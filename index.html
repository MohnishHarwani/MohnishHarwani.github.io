<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mohnish Harwani — Academic Portfolio</title>

  <style>
    body {
      margin: 0;
      background: #ffffff;
      color: #222;
      font-family: Arial, Helvetica, sans-serif;
      line-height: 1.6;
    }

    header {
      background: #f8f9fa;
      border-bottom: 1px solid #ddd;
      padding: 16px;
      text-align: center;
    }

    header h1 {
      margin: 0;
      font-size: 1.8rem;
      font-weight: 700;
    }

    nav a {
      margin: 0 10px;
      color: #0a66c2;
      text-decoration: none;
      font-weight: 600;
    }

    nav a:hover { text-decoration: underline; }

    .container {
      max-width: 800px;
      margin: 30px auto;
      padding: 0 20px;
    }

    .section {
      padding: 20px;
      border: 1px solid #e5e5e5;
      border-radius: 8px;
      background: #fafafa;
      margin-bottom: 25px;
    }

    h2 { margin-top: 0; }
    ul { padding-left: 18px; }
    a { color: #0a66c2; }
  </style>
</head>

<body>

<header>
  <h1>Mohnish Harwani, Home Page</h1>
  <nav>
    <a href="#about">About</a>
    <a href="#research">Research</a>
    <a href="#projects">Projects</a>
    <a href="#publications">Publications</a>
    <a href="#contact">Contact</a>
    <a href="assets/CV.pdf">CV</a>
  </nav>
</header>

<div class="container">

  <!-- ABOUT -->
  <section class="section" id="about">
    <h2>About Me</h2>
    <p>
      I am an undergraduate student at <strong>Purdue University</strong> majoring in
      <strong>Computer Science</strong> with a minor in Mathematics (GPA: 3.88).  
      My research focuses on developing <strong>training algorithms</strong> and
      <strong>architectures</strong>, that improve the performance of large-scale, deep neural networks.
      Recently, I've been trying to achieve this goal with with meta-learned neural network optimizers that can extract
      more generalizable and otherwise useful information from backpropogation gradients. I'm very interested in LLMs and reasoning.
    </p>
    <p>
    </p>
    <p>
      I am currently seeking <strong>PhD positions for Fall 2026</strong>.
    </p>
  </section>

  <!-- RESEARCH -->
  <section class="section" id="research">
    <h2>Research Experience</h2>

    <h3>Research Assistant — nanoHUB, Purdue University (Jan 2024 – Present)</h3>
    <ul>
      <li>Co-authored <strong>nanoHUB AURA</strong>, an LLM-powered scientific research assistant leveraging multi-agent architectures and access to 1M+ simulations/tools.</li>
      <li>Applied active learning to materials discovery, achieving a <strong>10× reduction in compute</strong> for materials screening (published in Computational Materials Science).</li>
      <li>Adapted GPT and Llama LLMs to execute scientific workflows, query datasets, and run simulations from natural language prompts.</li>
    </ul>

    <h3>CS 592 — Reinforcement Learning (2024–2025)</h3>
    <ul>
      <li>
        Developed a <strong>zeroeth-order optimizer</strong> using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for <strong>neural network warmup</strong>.  
        This optimizer significantly outperforms SGD in early-stage training, particularly in <strong>highly nonconvex</strong> and <strong>noisy</strong> optimization landscapes where gradient signals are unstable.  
        Demonstrated improved convergence and robustness during initial training phases of deep networks.
      </li>
    </ul>

    <h3>CS 577 — Natural Language Processing (2025–Present)</h3>
    <ul>
      <li>
        Adapted <strong>learned optimizers</strong> specifically for <strong>LLM pre-training</strong>.  
        By restricting the training distribution to LLM-style tasks and architectures, we meta-train a learned optimizer that <strong>outperforms Adam</strong> on small language models.  
        Demonstrated that task-specialization of learned optimizers improves stability and performance.
      </li>
    </ul>

    <h3>CS 573 — Data Mining (2025–Present)</h3>
    <ul>
      <li>
        Integrated <strong>Sharpness-Aware Minimization (SAM)</strong> into the training loop of learned optimizers to address core instability challenges in meta-learning.  
        Learned optimizers often overfit to specific architectures, datasets, or initialization distributions, limiting generalization.  
        Using SAM, we enforce updates that favor <strong>flatter minima</strong>, improving robustness under distribution shift.
      </li>

      <li>
        Built a meta-training pipeline that trains learned optimizers on diverse tasks (varying architectures, datasets, and initializations) while perturbing the optimization landscape using SAM-inspired worst-direction sharpening.  
        This reduces collapse modes and improves cross-task transfer ability.
      </li>

      <li>
        Demonstrated substantial improvements in:
        <ul>
          <li><strong>zero-shot transfer</strong> to unseen datasets,</li>
          <li><strong>robustness</strong> to initialization noise,</li>
          <li><strong>generalization</strong> to new architectures, such as CNNs and small transformers.</li>
        </ul>
      </li>

      <li>
        This work is motivated directly by my broader interest in <strong>stable, self-refining optimizers</strong> and connects to instability issues described in my SoP — specifically the brittleness of current deep learning optimization with respect to hyperparameters, architecture, and scale.
      </li>

      <li>
        This project is ongoing and aims to produce learned optimizers that scale more gracefully than existing approaches, ultimately contributing to the training of <strong>large-scale models</strong> where traditional optimization often fails.
      </li>
    </ul>

    <h3>Research Themes</h3>
    <p>
      My work is motivated by instability in modern deep learning pipelines and the need for 
      <strong>self-improving, scalable optimization algorithms</strong>.  
      I aim to develop techniques that integrate meta-learning, large-scale optimization, and 
      architectural design to build the next generation of optimizers and training systems for 
      foundation models.
    </p>
  </section>

  <!-- PROJECTS -->
  <section class="section" id="projects">
    <h2>Projects</h2>
    <ul>
      <li><strong>nanoHUB AURA</strong> — Large-scale LLM-powered scientific assistant; released as a <strong>ChemRxiv preprint</strong>. Uses multi-agent systems and physics-based simulation workflows to assist researchers at scale.</li>

      <li><strong>Zeroeth-Order Neural Optimizer</strong> — CMA-ES–based warmup optimizer outperforming SGD in early training for noisy, nonconvex landscapes.</li>

      <li><strong>SAM-Stabilized Learned Optimizers</strong> — Meta-training pipeline that integrates SAM into learned optimizer training, improving stability and transfer across architectures, tasks, and initializations.</li>

      <li><strong>KPEvents (Kaiser Permanente)</strong> — On-device AI-powered health calendar using Apple Foundation Models; generates personalized routines using medical records and HealthKit data, with strict privacy guarantees.</li>
    </ul>
  </section>

  <!-- PUBLICATIONS -->
  <section class="section" id="publications">
    <h2>Publications & Preprints</h2>
    <ul>
      <li>
        “<em>Accelerating active learning materials discovery with FAIR data and workflows</em>,”  
        <strong>Computational Materials Science</strong> (Elsevier), 2025.  
        M. Harwani, J.C. Verduzco, A. Strachan.  
        <a href="#">PDF</a>
      </li>

      <li>
        “<em>nanoHUB AURA: An Autonomous Universal Research Assistant for Scientific Computing</em>,”  
        <strong>ChemRxiv Preprint</strong>, 2024.  
        M. Harwani, A. Strachan, J.C. Verduzco, et al.  
        <a href="https://chemrxiv.org/engage/chemrxiv/article-details/691e4b62ef936fb4a28038d7" target="_blank">Preprint Link</a>
      </li>
    </ul>
  </section>

  <!-- CONTACT -->
  <section class="section" id="contact">
    <h2>Contact</h2>
    <p>Email: <a href="mailto:mharwan@purdue.edu">mharwan@purdue.edu</a></p>
    <p>GitHub: <a href="https://github.com/MohnishHarwani">github.com/MohnishHarwani</a></p>
    <p>LinkedIn: <a href="https://linkedin.com/in/mohnish-harwani-607b50291">linkedin.com/in/mohnish-harwani</a></p>
    <p>Resume: <a href="assets/Resume.pdf">Resume PDF</a></p>
  </section>

  <p style="text-align:center;color:#777;margin-top:20px;">
    © Mohnish Harwani — Hosted on GitHub Pages
  </p>

</div>

</body>
</html>
